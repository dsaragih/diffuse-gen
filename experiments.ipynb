{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "from utils_flexit import inception\n",
    "import torch\n",
    "from torchvision.transforms import functional as TF\n",
    "from guided_diffusion.guided_diffusion import dist_util\n",
    "from sklearn.cluster import KMeans\n",
    "import blobfile as bf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN_path = \"/home/daniel/divergent-nets/data/data_files/GAN_dict.pkl\"\n",
    "img_path = \"./cluster_samples0/cluster0-diffgen-2023-07-18-16-55/samples_5x256x256x4.pkl\"\n",
    "styled_img_path = \"./cluster_samples0/cluster0-diffgen-2023-07-18-16-55/styled_samples_5x256x256x4.pkl\"\n",
    "shape = (256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_key(key):\n",
    "    return f\".{key.split('diffusion-gen')[1]}\"\n",
    "\n",
    "def load_images_from_path(img_path, mask=False):\n",
    "    conversion = \"RGB\" if not mask else \"L\"\n",
    "    src_img_pil = Image.open(img_path).convert(conversion)\n",
    "    src_img_pil = src_img_pil.resize(shape, Image.LANCZOS)  # type: ignore\n",
    "    src_img = (\n",
    "        TF.to_tensor(src_img_pil).unsqueeze(0)\n",
    "    )\n",
    "    return src_img\n",
    "def prep_image_for_view(images, n=None):\n",
    "    paths = []\n",
    "    images_arr = []\n",
    "    iters = images.items() if n is None else list(images.items())[:n]\n",
    "    for path, imgs in iters:\n",
    "        paths.append(path)\n",
    "        images_arr.append(imgs)\n",
    "        batch_size = len(imgs)\n",
    "    images = np.concatenate(images_arr, axis=0)\n",
    "    # images = np.load(img_path)['arr_0']\n",
    "    print(images.shape) # (N, 256, 256, C)\n",
    "    images, masks = images[..., :3], images[..., 3]\n",
    "    # Erosion of mask\n",
    "    threshold = (masks.min() + masks.max()) / 2\n",
    "    print(\"Threshold\", threshold)\n",
    "    masks = np.where(masks > threshold, 255, 0).astype(np.uint8)\n",
    "\n",
    "    return batch_size, images, masks, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img_path, styled_img_path):\n",
    "    with open(img_path, \"rb\") as f:\n",
    "        images = pickle.load(f)\n",
    "    with open(styled_img_path, \"rb\") as f:\n",
    "        styled_images = pickle.load(f)\n",
    "    \"\"\"\n",
    "    images is a dictionary: {img_path: generated images}\n",
    "    generated images has shape (N, 256, 256, 4)\n",
    "    \"\"\"\n",
    "    iter = min(len(images), 5)\n",
    "    batch_size, images, masks, paths = prep_image_for_view(images, iter)\n",
    "    _, styled_images, styled_masks, _ = prep_image_for_view(styled_images, iter)\n",
    "    # kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (16, 16))\n",
    "    # ppmask = cv2.morphologyEx(masks[0], cv2.MORPH_OPEN, kernel)\n",
    "    # plt.imshow(ppmask, cmap=\"gray\")\n",
    "    #plt.imshow(images[-1])\n",
    "    # images = [Image.fromarray(np.uint8(img)) for img in images]\n",
    "    # masks = [Image.fromarray(np.uint8(m)) for m in mask]\n",
    "    for i in range(iter):\n",
    "        # Denoise image\n",
    "        # img =  cv2.fastNlMeansDenoisingColored(np.array(img), None, 10, 10, 7, 21)\n",
    "        # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        # img = Image.fromarray(img)\n",
    "        # Save the image\n",
    "        # img.save(f\"{img_dir}/{i}.png\")\n",
    "        # masks[i].save(f\"{img_dir}/{i}_mask.png\")\n",
    "        # Set axis off\n",
    "        path = shorten_key(paths[i // batch_size])\n",
    "        # path = paths[i // batch_size]\n",
    "        orig_img = load_images_from_path(path)\n",
    "        orig_mask = load_images_from_path(path.replace(\"masked-images\", \"masks\"))\n",
    "        print(bf.basename(paths[i // batch_size]))\n",
    "\n",
    "        # w, h = 3, 3\n",
    "        # dpi = 512\n",
    "        # Plot original image, inpainted image, styled image, then masks below\n",
    "        fig, ax = plt.subplots(2, 3, figsize=(10, 5), frameon=False)    \n",
    "        ax[0, 0].imshow(orig_img[0].permute(1, 2, 0))\n",
    "        # ax[0, 0].set_title(\"Original Image\")\n",
    "        ax[0, 0].axis(\"off\")\n",
    "        ax[0, 1].imshow(images[i])\n",
    "        # ax[0, 1].set_title(\"Inpainted Image\")\n",
    "        ax[0, 1].axis(\"off\")\n",
    "        ax[0, 2].imshow(styled_images[i])\n",
    "        # ax[0, 2].set_title(\"Styled Image\")\n",
    "        ax[0, 2].axis(\"off\")\n",
    "        ax[1, 0].imshow(orig_mask[0].permute(1, 2, 0))\n",
    "        # ax[1, 0].set_title(\"Original Mask\")\n",
    "        ax[1, 0].axis(\"off\")\n",
    "        ax[1, 1].imshow(masks[i], cmap=\"gray\")\n",
    "        # ax[1, 1].set_title(\"Inpainted Mask\")\n",
    "        ax[1, 1].axis(\"off\")\n",
    "        ax[1, 2].imshow(styled_masks[i], cmap=\"gray\")\n",
    "        # ax[1, 2].set_title(\"Styled Mask\")\n",
    "        ax[1, 2].axis(\"off\")\n",
    "        fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "show_image(img_path, styled_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(styled_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of the form chk_id_1_gen_scale_0_0_img.png\n",
    "def preproces_GAN_images(lst):\n",
    "    \"\"\"\n",
    "    First we want to keep only the first 348 images from the list. We then construct a dictionary of the form {img: list samples of img}\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    masked_res = {}\n",
    "    for i in range(len(lst)):\n",
    "        split = lst[i].split(\"_\")\n",
    "        id = int(split[2])\n",
    "        if id <= 348:\n",
    "            split[6] = \"0\"\n",
    "            key = \"_\".join(split)\n",
    "            mask_key = key.replace(\"img\", \"mask\")\n",
    "            res[key] = []\n",
    "            masked_res[mask_key] = []\n",
    "            for j in range(5):\n",
    "                split[6] = str(j)\n",
    "                # Check if in lst\n",
    "                if \"_\".join(split) in lst:\n",
    "                    res[key].append(\"_\".join(split))\n",
    "                    masked_res[mask_key].append(\"_\".join(split).replace(\"img\", \"mask\"))\n",
    "                else:\n",
    "                    print(f\"Missing image: {'_'.join(split)}\")\n",
    "        else:\n",
    "            break\n",
    "    return res, masked_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = os.listdir(\"../../data/synthetic_polyps_and_masks_10k_split/images\")\n",
    "# Sort lst based on split[2]\n",
    "lst.sort(key=lambda x: (int(x.split(\"_\")[2]), int(x.split(\"_\")[6])))\n",
    "res, masked_res = preproces_GAN_images(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(res), len(masked_res))\n",
    "print(list(res.keys())[0])\n",
    "print(list(masked_res.keys())[0])\n",
    "print(res[list(res.keys())[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_file = \"./guided_diffusion/segmented-images/masked-images/cju2yyhsp933j0855hp32e012.jpg\"\n",
    "mask_path = bf.join(bf.dirname(bf.dirname(rand_file)), 'masks', bf.basename(rand_file))\n",
    "\n",
    "print(\"Chosen file:\", bf.basename(rand_file))\n",
    "image_size = (256, 256)\n",
    "target_image_pil = Image.open(rand_file).convert(\"RGB\")\n",
    "target_mask_pil = Image.open(mask_path).convert(\"L\")\n",
    "target_image_pil = target_image_pil.resize(image_size, Image.LANCZOS)\n",
    "target_mask_pil = target_mask_pil.resize(image_size, Image.LANCZOS)\n",
    "\n",
    "# Dilate mask\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "mask_arr = np.array(target_mask_pil)\n",
    "mask_arr = np.where(mask_arr > 0, 1, 0).astype(np.uint8)\n",
    "dilated_mask = cv2.dilate(mask_arr, kernel, iterations=1)\n",
    "target_mask_pil = Image.fromarray(dilated_mask.astype(np.float64))\n",
    "\n",
    "target_image = (\n",
    "    TF.to_tensor(target_image_pil).unsqueeze(0).mul(2).sub(1)\n",
    ")\n",
    "#target_mask = TF.to_tensor(target_mask_pil)\n",
    "target_mask = (\n",
    "    TF.to_tensor(target_mask_pil).unsqueeze(0).mul(2).sub(1)\n",
    ")\n",
    "# Print shapes and extrema\n",
    "print(target_image.shape, target_mask.shape)\n",
    "print(target_image.min(), target_image.max())\n",
    "print(target_mask.min(), target_mask.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/daniel/divergent-nets/data/data_files/cc_samples_dil.pkl\", \"rb\") as f:\n",
    "    sample = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look through 10 images at a time, starting from {s}\n",
    "s = 210\n",
    "idxs = list(range(s, s + 10))\n",
    "for idx in idxs:\n",
    "    path, imgs = list(sample.items())[idx]\n",
    "    ims, masks = imgs[..., :3], imgs[..., 3]\n",
    "    # threshold = 127\n",
    "    # masks = np.where(masks > threshold, 255, 0).astype(np.uint8)\n",
    "    img, mask = ims[0], masks[0]\n",
    "    # kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (16, 16))\n",
    "    # mask = cv2.morphologyEx(masks[0], cv2.MORPH_OPEN, kernel)\n",
    "    w, h = 3, 3\n",
    "    dpi = 256\n",
    "    # Plot image and mask side by side\n",
    "    fig = plt.figure(figsize=(w, h), dpi=dpi, frameon=False)\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    # Reduce the blue channel\n",
    "    #img[..., 2] = np.where(img[..., 2] > 128, 100, img[..., 2])\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.imshow(mask, cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_orig(img_path):\n",
    "    # output_images_zip/cluster_styled_samples/images/cju5f26ebcuai0818xlwh6116_sample_0.jpg -> ./guided_diffusion/segmented-images/masked-images/cju5f26ebcuai0818xlwh6116.jpg\n",
    "\n",
    "    # Get the id\n",
    "    split = img_path.split(\"/\")[-1]\n",
    "    split = split.split(\"_\")\n",
    "    id = split[0]\n",
    "    # Get the original image\n",
    "    orig_img_path = f\"./guided_diffusion/segmented-images/masked-images/{id}.jpg\"\n",
    "    return orig_img_path\n",
    "\n",
    "def _list_image_files_recursively(data_dir):\n",
    "    results = []\n",
    "    for entry in sorted(bf.listdir(data_dir)):\n",
    "        full_path = bf.join(data_dir, entry)\n",
    "        ext = entry.split(\".\")[-1]\n",
    "        if \".\" in entry and ext.lower() in [\"jpg\", \"jpeg\", \"png\", \"gif\"]:\n",
    "            results.append(full_path)\n",
    "        elif bf.isdir(full_path):\n",
    "            results.extend(_list_image_files_recursively(full_path))\n",
    "    return results\n",
    "\n",
    "def form_dict(data_dir):\n",
    "    res = {}\n",
    "    for img_path in _list_image_files_recursively(data_dir):\n",
    "        img = load_images_from_path(img_path)\n",
    "        mask_path = img_path.replace(\"images\", \"masks\")\n",
    "        mask = load_images_from_path(mask_path, mask=True)\n",
    "        concat_img = np.concatenate((img, mask), axis=1)\n",
    "        # Transpose\n",
    "        concat_img = np.transpose(concat_img, (0, 2, 3, 1))\n",
    "        orig_img_path = get_matching_orig(img_path)\n",
    "        # print(orig_img_path)\n",
    "        # print(img_path)\n",
    "        # print(mask_path)\n",
    "        # print(concat_img.shape)\n",
    "        # print(orig_img_path)\n",
    "        if orig_img_path in res:\n",
    "            res[orig_img_path] = np.concatenate((res[orig_img_path], concat_img), axis=0)\n",
    "        else:\n",
    "            res[orig_img_path] = concat_img\n",
    "        \n",
    "    return res\n",
    "\n",
    "samples = form_dict(\"./output_imgs_zip/cluster_samples/images\")\n",
    "styled_samples = form_dict(\"./output_imgs_zip/cluster_styled_samples/images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(samples), len(styled_samples))\n",
    "print(list(samples.keys())[:5])\n",
    "print(list(samples.values())[0].shape, list(styled_samples.values())[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"For the samples in cluster_samples0, we want to combine them into one pkl file\"\"\"\n",
    "\n",
    "# Replace this with cluster_samples1, 2\n",
    "base_dir = \"./cluster_samples2/\"\n",
    "# Replace samples, styled_samples with loaded cc_samples, cc_styled_samples\n",
    "with open(\"./cluster_samples1/cc_samples.pkl\", \"rb\") as f:\n",
    "    samples = pickle.load(f)\n",
    "with open(\"./cluster_samples1/cc_styled_samples.pkl\", \"rb\") as f:\n",
    "    styled_samples = pickle.load(f)\n",
    "# samples = {}\n",
    "# styled_samples = {}\n",
    "# If starting from scratch, samples, styled_samples are empty dictionaries\n",
    "\n",
    "# Shorten the keys: /gpfs/fs1/home/p/ptyrrell/saragihd/diffusion-gen/guided_diffusion/segmented-images/masked-images/cju0s690hkp960855tjuaqvv0.jpg -> ./guided_diffusion/segmented-images/masked-images/cju0s690hkp960855tjuaqvv0.jpg\n",
    "def find_largest_sampling(base_dir):\n",
    "    largest = 0\n",
    "    for file in os.listdir(base_dir):\n",
    "        if file.endswith(\".pkl\"):\n",
    "            idx = 2 if file.startswith(\"styled\") else 1\n",
    "            size = int(file.split(\"_\")[idx].split(\"x\")[0])\n",
    "            if size > largest:\n",
    "                largest = size\n",
    "    return largest\n",
    "\n",
    "def shorten_key(key):\n",
    "    return f\".{key.split('diffusion-gen')[1]}\"\n",
    "\n",
    "# Purpose: We sample three images for each source image. We want to combine all the samples into one dictionary\n",
    "# So the entries in the dictionary are {source_image: samples} where samples is a numpy array of shape (3, 256, 256, 4)\n",
    "# 3 because we sample 3 images for each source image\n",
    "n_folders = 0\n",
    "for folder in os.listdir(base_dir):\n",
    "    if not os.path.isdir(f\"{base_dir}{folder}\"): continue\n",
    "    largest = 0\n",
    "    n_folders += 1\n",
    "    for file in os.listdir(f\"{base_dir}{folder}\"):\n",
    "        largest = find_largest_sampling(f\"{base_dir}{folder}\")\n",
    "        # Only open if its the largest sample file.\n",
    "        # e.g. there may be samples_50x256x256x4.pkl and samples_100x256x256x4.pkl\n",
    "        # We only want to open the latter\n",
    "        idx = 2 if file.startswith(\"styled\") else 1\n",
    "        if file.endswith(\".pkl\") and file.split(\"_\")[idx].split(\"x\")[0] == str(largest):\n",
    "            with open(f\"{base_dir}{folder}/{file}\", \"rb\") as f:\n",
    "                pkl_dict = pickle.load(f)\n",
    "                if file.startswith(\"styled\"): \n",
    "                    out = styled_samples\n",
    "                else: \n",
    "                    out = samples\n",
    "                for key in pkl_dict:\n",
    "                    shortened = shorten_key(key)\n",
    "                    if shortened not in out:\n",
    "                        out[shortened] = pkl_dict[key] # np array\n",
    "                    else:\n",
    "                        out[shortened] = np.concatenate((out[shortened], pkl_dict[key]), axis=0)\n",
    "                        \n",
    "\n",
    "print(n_folders)\n",
    "print(len(samples), len(styled_samples))\n",
    "print(list(samples.keys())[:5])\n",
    "print(list(samples.values())[0].shape, list(styled_samples.values())[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{base_dir}cc_samples.pkl\", \"wb\") as f:\n",
    "    pickle.dump(samples, f)\n",
    "with open(f\"{base_dir}cc_styled_samples.pkl\", \"wb\") as f:\n",
    "    pickle.dump(styled_samples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"./cluster_samples2/\"\n",
    "pkl_path = f\"{base_dir}cc_styled_samples\"\n",
    "with open(f\"{pkl_path}.pkl\", \"rb\") as f:\n",
    "    samples = pickle.load(f)\n",
    "\n",
    "out = {}\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (16, 16))\n",
    "for path, img_and_mask in samples.items():\n",
    "    imgs, masks = img_and_mask[..., :3], img_and_mask[..., 3]\n",
    "    masks = np.where(masks > 127, 255, 0).astype(np.uint8)\n",
    "    res_masks = []\n",
    "    for mask in masks:\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "        res_masks.append(mask)\n",
    "    res_masks = np.stack(res_masks, axis=0)\n",
    "    out[path] = np.concatenate([imgs, res_masks[..., None]], axis=-1)\n",
    "\n",
    "print(len(out))\n",
    "print(list(samples.values())[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{pkl_path}_dil.pkl\", \"wb\") as f:\n",
    "    pickle.dump(out, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/daniel/divergent-nets/data/data_files/cc_styled_samples_dil.pkl\", \"rb\") as f:\n",
    "    out = pickle.load(f)\n",
    "base_img = list(out.values())[19]\n",
    "img1 = base_img[0]\n",
    "img2 = base_img[1]\n",
    "img3 = base_img[2]\n",
    "# Plot side by side\n",
    "w, h = 6, 3\n",
    "dpi = 512\n",
    "\n",
    "fig = plt.figure(figsize=(w, h), dpi=dpi, frameon=False)\n",
    "\n",
    "ax = fig.add_subplot(2, 3, 1)\n",
    "ax.imshow(img1[..., :3])\n",
    "ax.axis(\"off\")\n",
    "ax = fig.add_subplot(2, 3, 2)\n",
    "ax.imshow(img2[..., :3])\n",
    "ax.axis(\"off\")\n",
    "ax = fig.add_subplot(2, 3, 3)\n",
    "ax.imshow(img3[..., :3])\n",
    "ax.axis(\"off\")\n",
    "# Plot masks\n",
    "ax = fig.add_subplot(2, 3, 4)\n",
    "ax.imshow(img1[..., 3], cmap=\"gray\")\n",
    "ax.axis(\"off\")\n",
    "ax = fig.add_subplot(2, 3, 5)\n",
    "ax.imshow(img2[..., 3], cmap=\"gray\")\n",
    "ax.axis(\"off\")\n",
    "ax = fig.add_subplot(2, 3, 6)\n",
    "ax.imshow(img3[..., 3], cmap=\"gray\")\n",
    "ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting below moved to separate notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_samples(img_path, idx=None):\n",
    "    with open(img_path, \"rb\") as f:\n",
    "        images = pickle.load(f)\n",
    "    paths = []\n",
    "    images_arr = []\n",
    "    for path, imgs in images.items():\n",
    "        choice = np.expand_dims(imgs[idx], axis=0) if idx is not None else imgs\n",
    "        paths.extend([path] * len(choice))\n",
    "        images_arr.append(choice)\n",
    "    images = np.concatenate(images_arr, axis=0)\n",
    "    # images = np.load(img_path)['arr_0']\n",
    "    images, masks = images[..., :3], images[..., 3]\n",
    "    images = torch.tensor(images.transpose(0, 3, 1, 2)).float()\n",
    "    masks = torch.tensor(masks).unsqueeze(1).float()\n",
    "    # If GAN images, remove this division\n",
    "    # images = images / 255.0\n",
    "    # masks = masks / 255.0\n",
    "    print(images.shape)\n",
    "    return images, masks, paths\n",
    "\n",
    "def load_GAN_images_from_dict(res, masked_res, k=0):\n",
    "    base_dir = \"/home/daniel/diff-seg/synthetic_polyps_and_masks_10k_split\"\n",
    "    images = []\n",
    "    masks = []\n",
    "\n",
    "    for key, val in res.items():\n",
    "        if len(val) > 0:\n",
    "            img_path = bf.join(base_dir, \"images\", val[k])\n",
    "            mask_key = key.replace(\"img\", \"mask\")\n",
    "            mask_path = bf.join(base_dir, \"masks\", masked_res[mask_key][k])\n",
    "            # Open image\n",
    "\n",
    "            image_pil = Image.open(img_path).convert(\"RGB\")\n",
    "            image_pil = image_pil.resize(shape, Image.LANCZOS)\n",
    "            image = (\n",
    "            TF.to_tensor(image_pil).unsqueeze(0)\n",
    "            )\n",
    "            images.append(image)\n",
    "            mask_pil = Image.open(mask_path).convert(\"L\")\n",
    "            mask_pil = mask_pil.resize(shape, Image.LANCZOS)\n",
    "            mask = (\n",
    "                TF.to_tensor(mask_pil).unsqueeze(0)\n",
    "            )\n",
    "            masks.append(mask)\n",
    "    images = torch.cat(images, dim=0)\n",
    "    masks = torch.cat(masks, dim=0)\n",
    "    return images, masks\n",
    "\n",
    "# Load source images\n",
    "def _list_image_files_recursively(data_dir):\n",
    "    results = []\n",
    "    for entry in sorted(bf.listdir(data_dir)):\n",
    "        full_path = bf.join(data_dir, entry)\n",
    "        ext = entry.split(\".\")[-1]\n",
    "        if \".\" in entry and ext.lower() in [\"jpg\", \"jpeg\", \"png\", \"gif\"]:\n",
    "            results.append(full_path)\n",
    "        elif bf.isdir(full_path):\n",
    "            results.extend(_list_image_files_recursively(full_path))\n",
    "    return results\n",
    "\n",
    "def load_images_from_dir(dir_path=\"./guided_diffusion/segmented-images/masked-images\", n=None):\n",
    "    src_path = dir_path\n",
    "\n",
    "    src_image_paths = _list_image_files_recursively(src_path)\n",
    "    if n is not None:\n",
    "        src_image_paths = src_image_paths[:n]\n",
    "    src_images = []\n",
    "    for path in src_image_paths:\n",
    "        src_img_pil = Image.open(path).convert(\"RGB\")\n",
    "        src_img_pil = src_img_pil.resize(shape, Image.LANCZOS)  # type: ignore\n",
    "        src_img = (\n",
    "            TF.to_tensor(src_img_pil).unsqueeze(0)\n",
    "        )\n",
    "        src_images.append(src_img)\n",
    "\n",
    "    src_images = torch.cat(src_images, dim=0).float()\n",
    "    print(src_images.shape)\n",
    "    return src_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_GAN_dict(res, masked_res):\n",
    "    image_files = _list_image_files_recursively(\"./guided_diffusion/segmented-images/masked-images\")\n",
    "    GAN_image_files = list(res.keys())\n",
    "    GAN_image_files.sort(key=lambda x: int(x.split(\"_\")[2]))\n",
    "    # Replace keys\n",
    "    out = {}\n",
    "    base_dir = \"../../data/synthetic_polyps_and_masks_10k_split\"\n",
    "    \n",
    "    for key, val in res.items():\n",
    "        images = []\n",
    "        masks = []\n",
    "        if len(val) > 0:\n",
    "            for k in range(len(val)):\n",
    "                img_path = bf.join(base_dir, \"images\", val[k])\n",
    "                mask_key = key.replace(\"img\", \"mask\")\n",
    "                mask_path = bf.join(base_dir, \"masks\", masked_res[mask_key][k])\n",
    "                # Open image\n",
    "\n",
    "                image_pil = Image.open(img_path).convert(\"RGB\")\n",
    "                image_pil = image_pil.resize(shape, Image.LANCZOS)\n",
    "                image = (\n",
    "                TF.to_tensor(image_pil).unsqueeze(0)\n",
    "                )\n",
    "                images.append(image)\n",
    "                mask_pil = Image.open(mask_path).convert(\"L\")\n",
    "                mask_pil = mask_pil.resize(shape, Image.LANCZOS)\n",
    "                mask = (\n",
    "                    TF.to_tensor(mask_pil).unsqueeze(0)\n",
    "                )\n",
    "                masks.append(mask)\n",
    "        images = torch.cat(images, dim=0).numpy()\n",
    "        images = images.transpose(0, 2, 3, 1)\n",
    "        masks = torch.cat(masks, dim=0).numpy()\n",
    "        masks = masks.transpose(0, 2, 3, 1)\n",
    "        # print(images.shape, masks.shape)\n",
    "        idx = GAN_image_files.index(key)\n",
    "        out[image_files[idx]] = np.concatenate([images, masks], axis=-1)\n",
    "\n",
    "    print(list(out.keys())[:5])\n",
    "    print(list(out.values())[0].shape)\n",
    "    with open(\"./GAN_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(out, f)\n",
    "_ = save_GAN_dict(res, masked_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, masks, paths = load_samples(\"./GAN_dict.pkl\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_images = load_images_from_dir(n=5)\n",
    "src_masks = load_images_from_dir(dir_path=\"./guided_diffusion/segmented-images/masks\", n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(src_images[0].permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/daniel/divergent-nets/data/data_files/cc_samples_dil.pkl\", \"rb\") as f:\n",
    "        images = pickle.load(f)\n",
    "\n",
    "with open(\"/home/daniel/divergent-nets/data/data_files/cc_styled_samples_dil.pkl\", \"rb\") as f:\n",
    "        styled_images = pickle.load(f)\n",
    "# Plot images on first row, mask on second\n",
    "# idxs = 59, 338, 342, 211\n",
    "idx = 211\n",
    "path, imgs = list(images.items())[idx]\n",
    "path, styled_imgs = list(styled_images.items())[idx]\n",
    "fig = plt.figure(figsize=(10, 5), frameon=False)\n",
    "# paths.extend([path])\n",
    "# imgs shape (N, 256, 256, 4)\n",
    "ims, masks = imgs[..., :3], imgs[..., 3]\n",
    "styled_ims, _ = styled_imgs[..., :3], styled_imgs[..., 3]\n",
    "# Plot imgs above masks\n",
    "original_image = os.path.join(\"./guided_diffusion/segmented-images/masked-images\", bf.basename(path))\n",
    "original_image_pil = Image.open(original_image).convert(\"RGB\")\n",
    "original_image_pil = original_image_pil.resize(shape, Image.LANCZOS)\n",
    "original_mask = os.path.join(\"./guided_diffusion/segmented-images/masks\", bf.basename(path))\n",
    "original_mask_pil = Image.open(original_mask).convert(\"L\")\n",
    "original_mask_pil = original_mask_pil.resize(shape, Image.LANCZOS)\n",
    "plot_length = len(ims) + 1\n",
    "\n",
    "ax = fig.add_subplot(2, plot_length, 1)\n",
    "ax.imshow(np.array(original_image_pil))\n",
    "ax.axis(\"off\")\n",
    "# ax = fig.add_subplot(2, plot_length, plot_length + 1)\n",
    "# ax.imshow(np.array(original_mask_pil), cmap=\"gray\")\n",
    "# ax.axis(\"off\")\n",
    "for i in range(1, plot_length):\n",
    "        ax = fig.add_subplot(2, plot_length, i + 1)\n",
    "        ax.imshow(ims[i-1])\n",
    "        ax.axis(\"off\")\n",
    "        ax = fig.add_subplot(2, plot_length, plot_length + i + 1)\n",
    "        # ax.imshow(masks[i-1], cmap=\"gray\")\n",
    "        ax.imshow(styled_ims[i - 1])\n",
    "        ax.axis(\"off\")\n",
    "        \n",
    "# Leave no space between subplots\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.savefig(f\"figures/styled{idx}.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display src_images, src_masks. Images in first row, masks in second row\n",
    "\n",
    "def show_data():\n",
    "    # Tight layout\n",
    "    fig = plt.figure(figsize=(16, 7), dpi=100, tight_layout=True, frameon=False)\n",
    "    # Add tight gridspec\n",
    "    gs = fig.add_gridspec(2, 5, wspace=0, hspace=0)\n",
    "\n",
    "    \n",
    "    for i in range(5):\n",
    "        ax = fig.add_subplot(gs[0, i])\n",
    "        ax.imshow(src_images[i].permute(1, 2, 0))\n",
    "        ax.axis(\"off\")\n",
    "        ax = fig.add_subplot(gs[1, i])\n",
    "        ax.imshow(src_masks[i].permute(1, 2, 0))\n",
    "        ax.axis(\"off\")\n",
    "show_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataloader\n",
    "batch_size = 3\n",
    "dataset = torch.utils.data.TensorDataset(src_images, src_images)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering moved to separate notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the images based on inception features\n",
    "def ret_img_mask(img_path):\n",
    "    img_dir = bf.dirname(bf.dirname(img_path))\n",
    "    mask_dir = bf.join(img_dir, \"masks\")\n",
    "\n",
    "    mask_path = bf.join(mask_dir, bf.basename(img_path))\n",
    "\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img = img.resize(shape, Image.LANCZOS)\n",
    "    mask = Image.open(mask_path).convert(\"RGB\")\n",
    "    mask = mask.resize(shape, Image.LANCZOS)\n",
    "    arr = np.array(img)\n",
    "    mask_arr = np.array(mask)\n",
    "    return arr, mask_arr\n",
    "\n",
    "\n",
    "def get_inception_features(img_paths):\n",
    "    dist_util.setup_dist()\n",
    "    inception_model = inception.InceptionV3()\n",
    "    inception_model = inception_model.to(dist_util.dev())\n",
    "\n",
    "    inception_model.eval()\n",
    "    inception_model.requires_grad_(False)\n",
    "    features_map = {}\n",
    "\n",
    "    for path in img_paths:\n",
    "        # Since inception only takes 3 channel inputs, we append the mask features to the image features\n",
    "        img, mask = ret_img_mask(path)\n",
    "        img, mask = TF.to_tensor(img).unsqueeze(0), TF.to_tensor(mask).unsqueeze(0)\n",
    "        img, mask = img.to(dist_util.dev()), mask.to(dist_util.dev())\n",
    "        feat = inception_model(img).squeeze().detach().cpu().numpy()\n",
    "        mask_feat = inception_model(mask).squeeze().detach().cpu().numpy()\n",
    "        feat = np.concatenate([feat, mask_feat], axis=0)\n",
    "\n",
    "        features_map[path] = feat\n",
    "    return features_map\n",
    "\n",
    "def create_clusters(features_map, num_clusters=10):\n",
    "    features = np.array(list(features_map.values()))\n",
    "    paths = np.array(list(features_map.keys()))\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(features)\n",
    "    clusters = {}\n",
    "    for i in range(num_clusters):\n",
    "        clusters[i] = paths[kmeans.labels_ == i]\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = src_image_paths[0]\n",
    "print(path)\n",
    "img, mask = ret_img_mask(path)\n",
    "fig = plt.figure(figsize=(10, 10), dpi=100, frameon=False)\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.imshow(img)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_map = get_inception_features(src_image_paths)\n",
    "# print(len(features_map))\n",
    "# print(len(features_map[src_image_paths[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters=20\n",
    "#clusters = create_clusters(features_map, num_clusters=n_clusters)\n",
    "with open('./clusters.pkl', 'rb') as f:\n",
    "    clusters = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of each cluster\n",
    "[len(clusters[i]) for i in range(n_clusters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample images from the clusters\n",
    "def sample_images_from_clusters(clusters, num_samples=1):\n",
    "    for j, cluster in enumerate(clusters.values()):\n",
    "        # if j != 2:\n",
    "        #     continue\n",
    "        paths = np.random.choice(cluster, num_samples if num_samples < len(cluster) else len(cluster), replace=False)\n",
    "        # Plot images in the same cluster in the same figure\n",
    "        w, h = 3, 3\n",
    "        dpi = 512\n",
    "        fig = plt.figure(figsize=(w, h), dpi=dpi, frameon=False)\n",
    "        for i, path in enumerate(paths):\n",
    "            ax = fig.add_subplot(1, num_samples, i + 1)\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            img = img.resize(shape, Image.LANCZOS)\n",
    "            ax.imshow(img)\n",
    "            ax.axis(\"off\")\n",
    "        # set title\n",
    "        plt.show()\n",
    "\n",
    "sample_images_from_clusters(clusters, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save clusters as pkl\n",
    "with open(\"./clusters.pkl\", \"wb\") as f:\n",
    "    pickle.dump(clusters, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiffuseIT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
